{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import scipy\n",
    "from scipy.optimize import lsq_linear\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal, halfnorm\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.linear_model import RidgeCV\n",
    "sys.path.append(r\"c:\\Users\\katie\\OneDrive\\Documents\\GitHub\\trial\")\n",
    "import PCA_Regress as pcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e3d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\J'\n",
      "C:\\Users\\katie\\AppData\\Local\\Temp\\ipykernel_2664\\3559308067.py:2: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  with open(base_path+'\\J_neu.pkl', \"rb\") as input_file:\n",
      "C:\\Users\\katie\\AppData\\Local\\Temp\\ipykernel_2664\\3559308067.py:12: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  with open(base_path+'\\J_mus.pkl', \"rb\") as input_file:\n"
     ]
    }
   ],
   "source": [
    "base_path =r\"c:\\Users\\katie\\OneDrive\\Desktop\\Thesis\"\n",
    "with open(base_path+'\\J_neu.pkl', \"rb\") as input_file:\n",
    "    J_pickle = pickle.load(input_file)\n",
    "del input_file\n",
    "\n",
    "file_path = os.path.join(base_path, 'N_neu.pkl')\n",
    "with open(file_path, \"rb\") as input_file:\n",
    "    N_pickle = pickle.load(input_file)\n",
    "del input_file\n",
    "\n",
    "base_path =r\"c:\\Users\\katie\\OneDrive\\Desktop\\Thesis\"\n",
    "with open(base_path+'\\J_mus.pkl', \"rb\") as input_file:\n",
    "    J_pickle_m = pickle.load(input_file)\n",
    "del input_file\n",
    "\n",
    "ile_path = os.path.join(base_path, 'N_mus.pkl')\n",
    "with open(ile_path, \"rb\") as input_file:\n",
    "    N_pickle_m = pickle.load(input_file)\n",
    "del input_file\n",
    "\n",
    "# base_path = \"/Users/kb6113/Desktop/Thesis\"\n",
    "# with open(base_path+'/J_neu.pkl', \"rb\") as input_file:\n",
    "#     J_pickle = pickle.load(input_file)\n",
    "# del input_file\n",
    "\n",
    "# with open(base_path+'/J_mus.pkl', \"rb\") as input_file:\n",
    "#     J_pickle_m = pickle.load(input_file)\n",
    "# del input_file\n",
    "\n",
    "J_all_tensor = J_pickle['J_all']['interpPSTH']\n",
    "J_M1_tensor = J_pickle['J_M1']['interpPSTH']\n",
    "J_PMd_tensor = J_pickle['J_PMd']['interpPSTH']\n",
    "J_mus_tensor = J_pickle_m['interpPSTH']\n",
    "J_mus_stack = np.vstack((J_mus_tensor, J_mus_tensor, J_mus_tensor, J_mus_tensor))\n",
    "\n",
    "\n",
    "N_all_tensor = N_pickle['N_all']['interpPSTH']\n",
    "N_M1_tensor = N_pickle['N_M1']['interpPSTH']\n",
    "N_PMd_tensor = N_pickle['N_PMd']['interpPSTH']\n",
    "N_mus_tensor = N_pickle_m['interpPSTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n",
      "(22,)\n",
      "(75,)\n"
     ]
    }
   ],
   "source": [
    "conds = 108 \n",
    "# 1. Create a range of all possible indexes\n",
    "all_idx = np.arange(conds)\n",
    "\n",
    "# 2. Shuffle the indexes to ensure random distribution across the splits\n",
    "# This prevents any inherent order in the data from biasing the splits.\n",
    "np.random.shuffle(all_idx)\n",
    "\n",
    "# 3. Calculate the sizes of each partition\n",
    "size_10_percent = int(np.round(conds * 0.1))\n",
    "size_20_percent = int(np.round(conds * 0.2))\n",
    "# The remainder goes to the 70% to ensure all indexes are used\n",
    "size_70_percent = conds - size_10_percent - size_20_percent\n",
    "\n",
    "# 4. Use numpy.split to divide the shuffled array\n",
    "# The indices array specifies the points where the array is split\n",
    "split_points = [size_10_percent, size_10_percent + size_20_percent]\n",
    "split_sets = np.split(all_idx, split_points)\n",
    "\n",
    "# 5. Assign the resulting arrays to their respective variables\n",
    "set_10_percent = split_sets[0]\n",
    "set_20_percent = split_sets[1]\n",
    "set_70_percent = split_sets[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_lam(mus_rank, neu_rank, time_bins):\n",
    "    \"\"\"\n",
    "    This function takes in the training data and will compute the best lambda value for ridge regression using cross-validation. It will return the best lambda\n",
    "    value and the mean squared error for that lambda.\n",
    "\n",
    "    Parameters: \n",
    "        mus_rank: a 2D numpy array of shape [ct, rank] which is the projection onto the first rank PCs\n",
    "        neu_rank: a 2D numpy array of shape [ct, rank] which is the projection onto the first rank PCs\n",
    "        M: the original muscle data of shape [conditions x time bins, muscles]\n",
    "\n",
    "    Returns: \n",
    "        best_lambda: the best lambda value found during cross-validation\n",
    "        mse: the mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "\n",
    "    # conditions in the sample \n",
    "    conds = int(neu_rank.shape[0] / time_bins)\n",
    "\n",
    "    # Define a range of lambda values to test\n",
    "    lambdas = np.logspace(-2, 3, 20)\n",
    "    \n",
    "    # Initialize variables to store the best lambda and its corresponding MSE\n",
    "    best_lambda = None\n",
    "    min_rmse = float('inf')\n",
    "\n",
    "    # listing and shuffling all possible indexes \n",
    "    all_idx = np.arange(conds)\n",
    "    np.random.shuffle(all_idx)\n",
    "\n",
    "    # calculate the sizes of each set \n",
    "    size_10_percent = int(np.round(conds * 0.1))    # 10% for lambda \n",
    "    size_20_percent = int(np.round(conds * 0.2))    # 20% for testing\n",
    "\n",
    "    # splitting the shuffled index array \n",
    "    split_points = [size_10_percent, size_10_percent + size_20_percent]\n",
    "    split_sets = np.split(all_idx, split_points)\n",
    "\n",
    "    # indexes of the split data \n",
    "    lam_idx = split_sets[0]           # 10% of data\n",
    "    test_idx = split_sets[1]          # 20% of data \n",
    "    train_idx = split_sets[2]         # 70% of data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    #split into testing and training, with a 20/80 split\n",
    "    test_size = int(np.round(0.2 * conds))\n",
    "    test_idx = np.random.choice(np.arange(conds), size = test_size, replace = False)\n",
    "    \n",
    "    # shaping back into a tensor \n",
    "    neu_tensor = pcar.shape_tensor(neu_rank, conds)\n",
    "    mus_tensor = pcar.shape_tensor(mus_rank, conds)\n",
    "\n",
    "    # isolating the test data \n",
    "    neu_test_tens = neu_tensor[test_idx, :, :]\n",
    "    mus_test_tens = mus_tensor[test_idx, :, :]\n",
    "\n",
    "    # indexes for training data \n",
    "    mask = np.ones(conds, dtype = bool)\n",
    "    mask[test_idx] = False\n",
    "    neu_training_tens = neu_tensor[mask, :, :]\n",
    "    mus_training_tens= mus_tensor[mask, :, :]\n",
    "\n",
    "    # shaping back into matrix for regression\n",
    "    neu_test = pcar.shape_matrix(neu_test_tens)\n",
    "    mus_test = pcar.shape_matrix(mus_test_tens)\n",
    "    neu_train = pcar.shape_matrix(neu_training_tens)\n",
    "    mus_train = pcar.shape_matrix(mus_training_tens)\n",
    "\n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for lam in lambdas:\n",
    "        # Fit a ridge regression model with the current lambda\n",
    "        W_hat= pcar.regress(mus_train, neu_train, lam)[0]\n",
    "\n",
    "        # Predict on the test sample\n",
    "        check = neu_test @ W_hat\n",
    "        \n",
    "        mse = pcar.mse_fun(mus_test, check)\n",
    "        mse_vals.append(mse)\n",
    "\n",
    "        rmse = np.sqrt(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "\n",
    "        # Update best lambda if current MSE is lower than previous minimu\n",
    "        \n",
    "        if rmse < min_rmse and lam != None:\n",
    "            min_rmse = rmse\n",
    "            best_lambda = lam\n",
    "    print(\">>> best_lam returning:\", best_lambda)\n",
    "    # Return the best lambda and its corresponding MSE         \n",
    "    return best_lambda, min_rmse\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wfh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
